---
layout: post
author: jai
Title: Language and BERT
categories: 
    -  Resources
tags:
    -  BERT
    -  NLP
    -  Transformers
---

Jacob Devlin's guest lecture at Stanford about [Bidirectional Encoder Representations from Transformers](https://arxiv.org/abs/1810.04805){:target="_blank"} (BERT), which was the SOTA Language model and a major paper.

<div class="video-container">
    <iframe width="420" height="315" src="https://www.youtube.com/embed/knTc-NQSjKA" frameborder="0" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

Implementation:  
[Huggingface](https://huggingface.co/models){:target="_blank"} is a great library and has a lot of pretrained Transformer models you can play around with or implement. [Simple Transformers](https://simpletransformers.ai/){:target="_blank"} is a simpler library for implementing and finetuning your own NLP model. It also lets you pull models from Huggingface.

For the latest State of the Art (SOTA) language models you can check benchmarks on [GLUE](https://gluebenchmark.com/leaderboard){:target="_blank"} and [SuperGLUE](https://super.gluebenchmark.com/leaderboard/){:target="_blank"}.
